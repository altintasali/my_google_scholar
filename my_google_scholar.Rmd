---
title: "Google Scholar Data of Ali Altıntaş"
author: "Ali Altıntaş"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    toc_depth: 3
    code_folding: show

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Getting started

## Required `R` libraries

```{r libraries, message=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load("rmdformats")
pacman::p_load("scholar", "rentrez")
pacman::p_load("data.table", "magrittr", "ggplot2", "openxlsx", "stringr", "RColorBrewer")
pacman::p_load("wordcloud", "tm")

```

## Google Scholar ID

It is essential to find your Google Scholar ID for this script to function. Simply, go to your page on [Google Scholar](https://scholar.google.com/) and click on **My Profile**. It will take you to your own Google Scholar page. For example, here is [my Google Scholar page](https://scholar.google.com/citations?user=rCdU5Z0AAAAJ&hl=en). 

The web address already shows the user ID:  *https&#58;//scholar.google.com/citations?user=***rCdU5Z0AAAAJ***&hl=en*

Therefore, let's assign user ID into an object:

```{r scholar_id}
## Define the id for Ali Altintas
id <- "rCdU5Z0AAAAJ"
```

# Data analysis

## Extracting data

```{r extract_info, results='hide'}
## Get profile
l <- get_profile(id)

## Print the name and affliation etc
l[1:7]
```
```{r extract_pubs, results='hide'}
## Get publication
p <- get_publications(id)

## Clean up the data
p_clean <- as.data.table(p)
p_clean <- p_clean[!(journal %in% "bioRxiv") & !(cid %in% "10083967249640088680")]
head(p_clean, 3)

```


```{r extract_citation, warning=FALSE}
## Get citation history, i.e. citations to his work in a given year
ct <- get_citation_history(id)

## Create annotations
text2plot <- data.table(`h-index = ` = l$h_index,
                        `i10-index = ` = l$i10_index,
                        `Articles = ` = nrow(p_clean), 
                        # `First author articles = ` = 6, 
                        `Articles without PhD supervisor = ` = p_clean[!grepl("Workman", p_clean$author)] %>% nrow,
                        # `Articles accepted for future publication = ` = 0,
                        `Average citation per article = ` = round(mean(p_clean$cites))
                        )
text2plot <- melt(text2plot)

## Write information to Excel
openxlsx::write.xlsx(text2plot, file = "my_google_schoar_stats.xlsx", asTable = TRUE)

```

## Abstracts

```{r extract_abstracts, warning=FALSE}
get_abstract <- function(paper_title, sleep = 1){
  Sys.sleep(sleep)
  #if(verbose){print(paste0("Article title provided: ", paper_title))}
  
  search_results <- tryCatch({
    rentrez::entrez_search(db = "pubmed", term = paper_title, retmax = 1)
  }, error = function(e) {
    message("An error occurred: ", e$message)
    return(NULL)
  })
  
  pmid <- search_results$ids[1][[1]]
  if(!is.null(pmid)){
    article_details <- rentrez::entrez_fetch(db = "pubmed", 
                                             id = pmid, 
                                             rettype = "abstract", 
                                             retmode = "text")  
    abstract <- stringr::str_split_fixed(string = article_details, 
                                         pattern = ".\n\n", 
                                         n = Inf) %>% trimws %>% as.vector
    auth_ind <- grep("^Author information:", abstract)
    comm_ind <- grep("^Comment in\n", abstract)
    copy_ind <- grep("^\u00A9|^Copyright \u00A9" , abstract) #(c) sign = \u00A9
    doi_ind <- grep("^DOI:", abstract)

    if(length(copy_ind)){
      ind <- doi_ind - 2
    }else{
      ind <- doi_ind - 1
    }
    
    abstract <- abstract[ind] %>% 
      stringr::str_replace_all(., "\n", " ") %>%
      stringr::str_replace_all(., "  ", " ")
    
    #if(verbose){print(abstract)}
    return(abstract)
  }else{
    message("No paper found for the given title.")
    return(NA)
  }

}

abstracts <- sapply(p_clean$title, get_abstract)
p_clean$abstract <- abstracts

write.xlsx(x = p_clean, file = "publication_details.xlsx", asTable = TRUE)
#fwrite(x = p_clean, file = "publication_details.tsv.gz", sep = "\t")
```

# Plots 

## Citations

```{r plot_citation}
## Add coordinates for ggplot
x_axis <- quantile(ct$year, 0.40)
y_axis1 <- quantile(cumsum(ct$cites), 0.90)
y_axis2 <- quantile(cumsum(ct$cites), 0.75)
text2plot[, x := x_axis]
text2plot[, y := seq(y_axis1, y_axis2, length.out = nrow(text2plot))]

## Plot citation trend
cite_plot <- ggplot(ct, aes(year, cumsum(cites))) + 
  geom_line() + 
  geom_point() + 
  theme_bw(base_size = 12) + 
  scale_x_continuous(breaks = scales::breaks_pretty()) + 
  scale_y_continuous(breaks = scales::breaks_pretty()) + 
  labs(title = paste0("Cumulative citations of ", 
                      l$name), 
       subtitle = paste0("Data retrieved from Google Scholar on", 
                         format(Sys.time(), "%e %B %Y, %A"))) + 
  xlab("Year") + 
  ylab("Citations") + 
  ggplot2::annotate("text", 
           x = text2plot$x, 
           y = text2plot$y, 
           label = paste(text2plot$variable,text2plot$value), 
           hjust  = 1)

ggsave(filename = "my_google_scholar_citation.png", 
       plot = cite_plot, 
       width = 8, 
       height = 5)

cite_plot
```

## Abstracts

```{r plot_abstract-wordcloud}
docs <- Corpus(VectorSource(c(p_clean$title, p_clean$abstract)))
docs <- tm_map(docs, content_transformer(tolower))  
docs <- tm_map(docs, removePunctuation)  
#docs <- tm_map(docs, removeNumbers) 
docs <- tm_map(docs, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
word_freqs <- sort(rowSums(matrix), decreasing=TRUE)
dt <- data.frame(word=names(word_freqs), freq=word_freqs)

palette <- brewer.pal(8, "Dark2")
set.seed(666)
wordcloud(words = dt$word, 
          freq = dt$freq, 
          scale = c(2.5, .5),
          min.freq = 10, 
          max.words = 100,
          random.order = FALSE, 
          colors=palette, 
          random.color = TRUE)
```

# Session Info

```{r session_info}
sessionInfo()
```

